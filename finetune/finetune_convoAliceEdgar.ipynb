{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95414fb9-40fc-4e7d-a4e7-98374f98cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have 2 models to talk each other after fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083f8359-1a51-408e-9a1d-41fda76814d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchinfo import summary\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66999745-9508-4ed9-ae8f-c04f1b37c7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading weights: 100%|█| 160/160 [00:00<00:00, 1579.92it/s, Materializing param=\n",
      "\u001b[1mGPTNeoForCausalLM LOAD REPORT\u001b[0m from: EleutherAI/gpt-neo-125m\n",
      "Key                                                   | Status     |  | \n",
      "------------------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
      "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Loading weights: 100%|█| 160/160 [00:00<00:00, 1924.12it/s, Materializing param=\n",
      "\u001b[1mGPTNeoForCausalLM LOAD REPORT\u001b[0m from: EleutherAI/gpt-neo-125m\n",
      "Key                                                   | Status     |  | \n",
      "------------------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
      "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Eletuther's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n",
    "\n",
    "# load in 2 GPTneos and push to GPU\n",
    "modelAlice = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "modelEdgar = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "modelAlice = modelAlice.to(device)\n",
    "modelEdgar = modelEdgar.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f375c0-143e-4da0-8e74-86b90ab90b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52954 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# throught the looking glass (aka alice in wonderland)\n",
    "text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n",
    "aliceTokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "# edgar allan Poe\n",
    "text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n",
    "edgarTokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb62ab1-d884-42a9-a0dd-4d0dd29e453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALICE optmizer\n",
    "optimizerAlice = torch.optim.AdamW(modelAlice.parameters(), lr=5e-5, weight_decay=.01)\n",
    "optimizerEdgar = torch.optim.AdamW(modelEdgar.parameters(), lr=5e-5, weight_decay=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94603a7-94b9-4dd7-800c-589725ddd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 16\n",
    "num_samples = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8add14b4-8a7d-4273-b326-5c32431af07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 0/100, losses (Alice/eDgar): 2.551490068435669 / 2.704116106033325\n",
      "Sample: 25/100, losses (Alice/eDgar): 1.97757887840271 / 2.6722307205200195\n",
      "Sample: 50/100, losses (Alice/eDgar): 1.627395749092102 / 2.6514389514923096\n",
      "Sample: 75/100, losses (Alice/eDgar): 1.4499162435531616 / 2.6057238578796387\n"
     ]
    }
   ],
   "source": [
    "tokenProbs = np.zeros((num_samples,3))\n",
    "\n",
    "lossAlice = np.zeros(num_samples)\n",
    "lossEdgar = np.zeros(num_samples)\n",
    "\n",
    "for sampli in range(num_samples):\n",
    "    # init batch losses to accumulate\n",
    "\n",
    "    # ALICE fine tuning\n",
    "    # get a batch of data\n",
    "    ix = torch.randint(len(aliceTokens)-seq_len, size = (batch_size,))\n",
    "    X = aliceTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n",
    "\n",
    "    #fwd pass and get loss\n",
    "    modelAlice.zero_grad()\n",
    "    outputs = modelAlice(X, labels=X)\n",
    "\n",
    "    # backprop and store loss\n",
    "    outputs.loss.backward()\n",
    "    optimizerAlice.step()\n",
    "    lossAlice[sampli] = outputs.loss.item()\n",
    "\n",
    "\n",
    "    #EDGAR fine tuning\n",
    "    ix = torch.randint(len(edgarTokens)-seq_len, size = (batch_size,))\n",
    "    X = edgarTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n",
    "\n",
    "    #fwd pass and get loss\n",
    "    modelEdgar.zero_grad()\n",
    "    outputs = modelEdgar(X, labels=X)\n",
    "\n",
    "    # backprop and store loss\n",
    "    outputs.loss.backward()\n",
    "    optimizerAlice.step()\n",
    "    lossEdgar[sampli] = outputs.loss.item()\n",
    "\n",
    "    if sampli%25==0:\n",
    "        print(f'Sample: {sampli}/{num_samples}, losses (Alice/eDgar): {lossAlice[sampli]} / {lossEdgar[sampli]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c4055-c4d9-4be0-ba91-1909be71f7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50e583f-8a0f-40b9-9be7-8bf7bd34dc43",
   "metadata": {},
   "source": [
    "Have the models chat with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c10ef0-6ebd-4548-ac98-48a9299dfc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "** Alice says:\n",
      " Hello, my name is Alice.\n",
      "\n",
      "\n",
      "** Edgar says (total token count: 57):\n",
      "  I'm a small boy who can read the signs and write messages. I'm a very busy man, living up to 20 years old. However, I am currently living in a home. I'm not quite that interested in playing games, nor do\n",
      "\n",
      "\n",
      "** Alice says (total token count: 107):\n",
      "  I want to draw a picture, so I sit and listen.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER IX.\n",
      "The Mock Turtle\n",
      "\n",
      "Alice thought most of the day after dark—after work, in fact—\n",
      "\n",
      "\n",
      "** Edgar says (total token count: 157):\n",
      " had started. Her brother, Bob, had been with her ever since they grew up. He'd always been very, very interested in what she could do to help him and she'd seen him as a different person to him and this made him change\n",
      "\n",
      "\n",
      "** Alice says (total token count: 207):\n",
      "  all the time: he'd always been a small fellow, with a large goatee and brown eyes, and he wore his tie the right size for all that makes him look so small and round; and it always struck her that Alice had always\n",
      "\n",
      "\n",
      "** Edgar says (total token count: 257):\n",
      "  been interested in what made him look so small and round, and she'd been very close to being interested in her brother if there was anything around him.\n",
      "\n",
      "There'd always been these two of them, standing just outside Alice's front\n",
      "\n",
      "\n",
      "** Alice says (total token count: 307):\n",
      "  door: they were talking among themselves; if you weren't very nervous about them going in, you could guess what they were saying, but Alice wasnít nearly so nervous as she usually was, so she couldn't help thinking about them, and\n",
      "\n",
      "\n",
      "** Edgar says (total token count: 357):\n",
      "  she told her brother about the letter Bob had written her and it came down the end: she read it on a Sunday morning in the morning. He and Bob had taken the letter and read it out into the house and Alice read it and it was\n",
      "\n",
      "\n",
      "** Alice says (total token count: 407):\n",
      "  exactly as it had come to be; there were three lines in it, all of them very nicely done, running down the middle: it was very elegant, very funny, and she certainly could not have been more than two or three pups this\n",
      "\n",
      "\n",
      "** Edgar says (total token count: 457):\n",
      "  morning.\n",
      "\n",
      "Her brothers had been good friends and the letters had said something about the letter Bob had written her and of Alice. And with the end of the letter, for a moment, Alice was afraid for Bob, for Bob's\n",
      "\n",
      "\n",
      "** Alice says (total token count: 507):\n",
      "  feelings were always mixed up with hers, so she tried her hardest at being afraid of him, she thought, for fear of giving him away as soon as she could, but he didn't seem to believe in being liked; and then, quite suddenly\n"
     ]
    }
   ],
   "source": [
    "# kick off the convo\n",
    "outAlice = tokenizer.encode('Hello, my name is Alice.', return_tensors = 'pt').to(device)\n",
    "print('\\n\\n** Alice says:\\n', tokenizer.decode(outAlice[0].cpu()))\n",
    "\n",
    "for _ in range(5):\n",
    "    #Edgars turn\n",
    "    outEdgar = modelEdgar.generate(outAlice, max_new_tokens = 50, do_sample=True, pad_token_id=50257)\n",
    "    print(f'\\n\\n** Edgar says (total token count: {len(outEdgar[0])}):\\n',\n",
    "        tokenizer.decode(outEdgar[0][len(outAlice[0]):].cpu()))\n",
    "\n",
    "    # alice turn\n",
    "    outAlice = modelAlice.generate(outEdgar, max_new_tokens = 50, do_sample=True, pad_token_id=50257)\n",
    "    print(f'\\n\\n** Alice says (total token count: {len(outAlice[0])}):\\n',\n",
    "        tokenizer.decode(outAlice[0][len(outEdgar[0]):].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a5898-4b96-452e-9db3-9dbc3126657f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
