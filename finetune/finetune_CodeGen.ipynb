{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202b9487-fe6c-4c7a-a2b2-10099326d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this introduces Salesforce's CodeGen model which is a pretrained \n",
    "# model for generating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4119bb1e-a974-425c-bc65-41ae2a0ee5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device =  torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648b117-2823-492c-b1dc-8d19a646c61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84c59e8a-f6e9-45ea-8374-47102eba684f",
   "metadata": {},
   "source": [
    "Import and inspect the CodeGen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a92b869c-ede4-42b3-b09b-b56b7a037bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|â–ˆ| 165/165 [00:00<00:00, 1876.00it/s, Materializing param=\n",
      "\u001b[1mCodeGenForCausalLM LOAD REPORT\u001b[0m from: Salesforce/codegen-350M-mono\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "transformer.h.{0...19}.attn.causal_mask | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Model source\n",
    "# https://huggingface.co/Salesforce/codegen-350M-mono   [this model is only trained on Python - mono]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-350M-mono')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-350M-mono').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf11ecfc-5f0d-4cf6-94d0-72ba30a5c3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad356fdc-56eb-497d-bb43-fdf24fe31f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenForCausalLM(\n",
       "  (transformer): CodeGenModel(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-19): 20 x CodeGenBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CodeGenAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): CodeGenMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "# the model has 50k tokens, and embedd dimension in 1024\n",
    "# it doesnt have position embeddings\n",
    "# it has  20 transformer blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378769e-a9ed-4dbe-85a1-9361ef7f03c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b66f2352-8021-42ac-84c1-0064c92490be",
   "metadata": {},
   "source": [
    "Check out the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099a5bd0-c7eb-4592-8a27-cb1e00bb824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has 50257 tokens\n",
      "Token 21223 is \"affe\"\n",
      "Token  5724 is \" rot\"\n",
      "Token  6110 is \" defensive\"\n",
      "Token 36797 is \" haste\"\n",
      "Token  6248 is \"sen\"\n",
      "Token  5545 is \" presented\"\n",
      "Token 21230 is \"safe\"\n",
      "Token  9617 is \" threw\"\n",
      "Token 18861 is \"Bob\"\n",
      "Token  7010 is \" isol\"\n",
      "Token 39929 is \" Reese\"\n",
      "Token 37382 is \"addle\"\n",
      "Token  4797 is \"iliar\"\n",
      "Token  6828 is \"ounter\"\n",
      "Token 23691 is \" Push\"\n",
      "Token 35794 is \" Latvia\"\n",
      "Token 42605 is \" Rita\"\n",
      "Token 42791 is \" Refugee\"\n",
      "Token 49166 is \" analytic\"\n",
      "Token  3402 is \" shown\"\n",
      "Token  5980 is \" dial\"\n",
      "Token 38559 is \" LIC\"\n",
      "Token 48494 is \"Whereas\"\n",
      "Token 24590 is \" laps\"\n",
      "Token 22245 is \"Richard\"\n",
      "Token  9079 is \" requirement\"\n",
      "Token 45702 is \" grinning\"\n",
      "Token 44767 is \" bulldo\"\n",
      "Token 36158 is \" Augustine\"\n",
      "Token 10365 is \"opp\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Tokenizer has {tokenizer.vocab_size} tokens')\n",
    "\n",
    "for i in range(30):\n",
    "    randtok = torch.randint(tokenizer.vocab_size,(1,))\n",
    "    # print(randto)\n",
    "    print(f'Token {randtok[0]:5} is \"{tokenizer.decode(randtok)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753bf6fc-bc3e-4c33-b3f0-f06cd7566bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10365])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randtok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce60b04-c8be-4488-b5cb-27eb2b9b3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer has 50257 tokens in its vocab\n",
    "# But the model's embedding matrix is of size 51200x1024 (not 50257)\n",
    "# embeding matrix larger than the vocab size of tokenizer\n",
    "     # that means there are several rows in the embedd matrix that have no connection to the data\n",
    "     #they get randomly modified during backprop, but dont correspond to any tokens\n",
    "     # maybe to fit the matrix in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a2f60-4547-460a-a48e-9fc53eb4336d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c4191a-2ffc-4f90-a592-8c94a00a8131",
   "metadata": {},
   "source": [
    "Genereate a response to some code input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4cb552c-fde6-4c1c-b3e7-10f72d78ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i in range(10):\n",
      "            bounds = self.Bounds.allocateOne(self.n_agents)\n",
      "            Bounds.constrains(i)[0]).attach(self.aloc_d.location_vector() == (randn()) | bounds.location.getRotationalFrame(),i).draw(*bounds,2,2,'+',(False,True.5),self.placements.clear())##,color=[0.1,0.2,0.3]+1e9)\n",
      "            # for loc in ['r', None, 10.4*i] do B.n_aloc_dim(s[loc+'s'].getNumActions) if true\n",
      "        Bounds.boundsFrom('r', (False,True))\n",
      "        Bounds(s1.agent, loc=10).setOrientalMatrix((False,True))\n",
      "        Boms = [BoxMotionSaver('rzw',\n",
      "                                 n_domains=s\n"
     ]
    }
   ],
   "source": [
    "text = 'for i in range(10):'\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "generated_ids = model.generate(input_ids, max_length=222,temperature=1.4, do_sample=True)\n",
    "print(tokenizer.decode(generated_ids[0],skip_special_tokens=True))\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32a978-e72f-4f50-ad68-541d2bb88214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0daecaa3-7895-4d69-abbe-568a0caf0630",
   "metadata": {},
   "source": [
    "Now import git-calc files from github so that we can finetune the model on that codebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce15c60-1e48-434e-b419-d0b5dd935e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import nbformat\n",
    "\n",
    "# download the repo\n",
    "repo_url = 'htpps://github.com/mikexcohen/Calculus_book.git'\n",
    "repo_dir = 'Calci"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
