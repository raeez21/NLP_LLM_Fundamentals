{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa16c95e-c092-4589-b31e-9954478b9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use BERT model and add a final classification layer to classify into 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae425772-78a9-4ea1-b9b8-dd0a49f08589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets huggingface_hub fsspec\n",
    "from datasets import load_dataset, DatasetDict\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e474789-eabf-48d2-b128-c52a6e565e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ad6082-a81d-460c-b6db-d6f623db2b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad02f7517b0b49cbb4ea0b6e1d034c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1b5241-1830-4b46-941c-60d659c45bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664d8718-e471-42ad-a6db-17205443176d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert\n",
    "# note the namings are diff to OpenAI\n",
    "# we have a pooler layer as final layer which has 768x768, not the vocab_size, bcoz this is for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce06d56-8d79-412d-8708-932d948c139c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.embeddings.dropout.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566f5cb-f7a4-49c5-8128-c14220843849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82616573-dad4-4bf5-899d-6ef13d386683",
   "metadata": {},
   "source": [
    "More on model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e25885-a0cd-4f3e-a62e-9541cff3fdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5672, 2033, 2007, 2151, 3793, 2017, 2066, 1012,  102]],\n",
       "       device='mps:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Replace me with any text you like.'\n",
    "tokens = tokenizer(text, return_tensors='pt').to(device)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f68b88-a049-4c85-a4a2-8b51e913574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model output using sepcific inputs\n",
    "output = bert(\n",
    "    input_ids = tokens['input_ids'],\n",
    "    attention_mask = tokens['attention_mask']\n",
    ")\n",
    "\n",
    "# better way\n",
    "output = bert(**tokens) #dict unpacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b6c631-c688-4fc0-b07c-cbb6b80fb322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'last_hidden_state',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pooler_output',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1f8e16-a72d-4c9a-9f6b-7841a5364961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35183fa5-19a9-43bc-99d4-1f6875ff1826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape #output of final linear layer of bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbdc8d96-7314-49da-a41c-ce8e21747ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# can we get text output?  NOOOO bcoz this is not for text gen\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(tokens, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1965\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1964\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1965\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1967\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "# can we get text output?  NOOOO bcoz this is not for text gen\n",
    "bert.generate(tokens, max_length=100,do_sample=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25478ae9-ccd8-432a-a371-e4878b565124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "989226dd-3c15-4de8-b320-20bc277f084d",
   "metadata": {},
   "source": [
    "CREATE an LLM model using pretrained BERT with a new head that does the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a535711-51fa-4228-9cc8-89ff145e1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForBinaryClassification(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForBinaryClassification, self).__init__()\n",
    "\n",
    "        # Load the pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        #classification head that converts 678-d pooled output into 2 final outuputs\n",
    "        self.classifier = nn.Linear(768,2)\n",
    "        self.dropout = nn.Dropout(self.bert.embeddings.dropout.p) #10%\n",
    "\n",
    "        #init the weights and biases\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "        # fwd pas through the downloaded(pretrained) BERT\n",
    "        outputs = self. bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_idsn = token_type_ids)\n",
    "\n",
    "        # extract the pooled output and apply dropout\n",
    "        pooled_output  = self.dropout(outputs.pooler_output)\n",
    "\n",
    "        # final push through classification layer\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee2ead5-cd42-4258-a760-f463f8e2ef8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb00d97f62de47829e52bc61ccd94fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5023, -0.4966]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of model and test it\n",
    "model = BertForBinaryClassification().to(device)\n",
    "\n",
    "tokens = tokenizer(text, return_tensors='pt').to(device)\n",
    "out = model(**tokens)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9417e80-c76a-4b71-9a2d-56433c43a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output of the model is not nexxt token prediction\n",
    "# its a binary classification\n",
    "# now we train the model, to correspond this output to positive and -ve reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf519e99-6ddf-42b3-b409-e69bfa98a078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "731e84b8-5bc9-4044-b930-37e28f53c99a",
   "metadata": {},
   "source": [
    "Import the datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "659e06ba-5637-421d-847f-4df0b81a0355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "# this is a special dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7617d02c-1361-4943-bcb3-2ef4ff7528b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"S.S. Van Dine must have been a shrewd businessman in dealing with Hollywood. Most of the film series' from the studio days were usually confined to one or two studios. But apparently Van Dine must have sold his rights to each book about Philo Vance one at a time. Note that Paramount, MGM, Warner Brothers, and more all released Philo Vance films. Only Tarzan seemed to get around Hollywood more.<br /><br />MGM produced the Garden Murder Case and starred Edmund Lowe as the fashionable detective. Of course MGM had the screen's original Philo under contract at the time, but Bill Powell was busy doing The Thin Man at the time and I guess Louis B. Mayer decided to concentrate him there.<br /><br />Edmund Lowe is a pretty acceptable Philo Vance. Lowe had started out pretty big at the tail end of the silent era with What Price Glory and then with a string of films with Victor McLaglen with their Flagg and Quirt characters. But after McLaglen got his Oscar for The Informer, Lowe seemed to fade into the B picture market. <br /><br />The Garden Murder Case involves three separate victims, Douglas Walton, Gene Lockhart, and Frieda Inescourt. The sinister atmosphere around the perpetrator kind of gives it away, the mystery is really how all the killings are connected and how they are accomplished.<br /><br />I will say this though. Vance takes a very big chance in exposing the villain and the last 15 minutes are worthy of Hitchcock.\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][24000]\n",
    "# we have labels 0 and 1 -ve and +ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aedbf-7bd8-48fd-a5f9-1b05c0a0e778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5aa9d7-ee94-486f-90ae-af758488e1c0",
   "metadata": {},
   "source": [
    "its a lot of data, lets take a small sample and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596988a7-712f-42a3-b349-8b154b9a00e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this special dict has a method called select which selects data points\n",
    "dataset['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389d4a21-5add-49cb-9fc5-07818ca12103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset['train'] ois organised in a way that first half is all 0s and next half all 1s\n",
    "# so we need to reduce dataset size while:\n",
    "#    1) including both categories\n",
    "#    2) preserving only 'train' and 'test (dont use unsupervised)\n",
    "smalldata = DatasetDict({split: dataset[split].select(range(10000,15000)) for split in ['train','test']})\n",
    "smalldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb47c5d-ba9e-437b-9e70-36fc8b30feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf8d184-5db0-4edb-b45c-3f91ed3197e6",
   "metadata": {},
   "source": [
    "Tokenizing the text with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77daa9ab-ba33-4439-bb4c-b1e0a89423a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this text, we have variable sequences\n",
    "# we need to deal with this because LMs not designed to handle batches with seq of diff lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "260597bb-c2b1-45fa-a513-cba703f71863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Naive' tokenization (N=363):\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n",
      "\n",
      "Better tokenization (N=512):\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# this works....\n",
    "first_try = tokenizer(dataset['train'][0]['text'])\n",
    "\n",
    "# but this is better bcz reviews have diff lengths\n",
    "better  = tokenizer(\n",
    "    dataset['train'][0]['text'], # the text to tokenize\n",
    "    max_length = 512,\n",
    "    padding = 'max_length', # usning pad_token to reach max_len\n",
    "    truncation=True) # cut out tokens > max_len\n",
    "\n",
    "print(f\"'Naive' tokenization (N={len(first_try['input_ids'])}):\")\n",
    "print(f\"{first_try['input_ids']}\")\n",
    "\n",
    "print(f\"\\nBetter tokenization (N={len(better['input_ids'])}):\")\n",
    "print(f\"{better['input_ids']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed9962a7-e89c-4e9c-a89b-328e248d137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a token func that processes each data sample\n",
    "def tokenize_function(one_sample):\n",
    "    return tokenizer(\n",
    "        one_sample['text'],\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True)\n",
    "\n",
    "# apply tokeniation func to dataset (batched for efficiency\n",
    "tokenized_dataset = smalldata.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc083bbb-cacf-4fc1-ae90-d8f4a58230d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "204c55a4-603c-4b5b-b44f-4e4bdfef45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text pair\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# change format to pytorch tensors\n",
    "tokenized_dataset.set_format('torch',columns=['input_ids', 'attention_mask', 'label','token_type_ids'])\n",
    "\n",
    "# create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(tokenized_dataset['test'], shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc2d5faa-ee02-4b9d-a27b-1b68653bbb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "762d31c4-cc24-4cc8-9ae0-0b7a2e725129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]),\n",
       " 'input_ids': tensor([[  101,  2038,  2045,  ...,     0,     0,     0],\n",
       "         [  101,  2044,  2633,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2028,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1045,  2442,  ...,  2466,  1997,   102],\n",
       "         [  101,  2703, 21863,  ...,     0,     0,     0],\n",
       "         [  101,  2023,  2003,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = next(iter(train_dataloader))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4756988a-822f-4d08-a109-5e99575044e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(X['input_ids'].shape)\n",
    "print(X['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937117a-8d55-44dc-a354-f0cc0bccb9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3167a0b-14bf-4178-aefa-b72f2fcaffbc",
   "metadata": {},
   "source": [
    "Now fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cb1c75e-7b86-436f-99be-6969386d58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fun = nn.CrossEntropyLoss() # cross entropy for multi class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5157d0d-e8cc-4cb9-8592-af7255a6a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a bathc of data\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# move to GPU\n",
    "tokenz = batch['input_ids'].to(device)\n",
    "attn_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['label'].to(device)\n",
    "\n",
    "# clear the prev grads\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# fwd pass and get model preds\n",
    "logits = model(tokenz, attention_mask = attn_mask)\n",
    "predLabels = torch.argmax(logits, dim=1)\n",
    "\n",
    "# calculate and store loss+avg accuracy\n",
    "loss = loss_fun(logits, labels)\n",
    "train_accuracy = (predLabels == labels).sum().item() / train_dataloader.batch_size\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f322088f-152d-4fe7-a7b7-823704a47b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits are of size torch.Size([16, 2]) and are:\n",
      " tensor([[ 0.4685,  0.1722],\n",
      "        [ 1.1158,  0.4824],\n",
      "        [ 0.9499, -0.0326],\n",
      "        [ 0.6029,  0.3132],\n",
      "        [ 0.6427,  0.1149],\n",
      "        [ 0.2148,  0.4113],\n",
      "        [ 0.6283, -0.7811],\n",
      "        [ 0.1233, -0.0621],\n",
      "        [ 0.2147,  0.4677],\n",
      "        [ 0.5287, -0.4137],\n",
      "        [ 1.1133,  0.8755],\n",
      "        [ 0.5969, -0.1799],\n",
      "        [ 0.9275,  0.1943],\n",
      "        [ 0.4733, -0.1009],\n",
      "        [ 0.3188,  0.7691],\n",
      "        [ 0.9253, -0.0859]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'logits are of size {logits.shape} and are:\\n', logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcdd5971-66b4-46a7-8f85-7d0b98badf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model preds: tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], device='mps:0')\n",
      "True labels: tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print('Model preds:', predLabels)\n",
    "print('True labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1adaa7f-a02a-4afd-82ac-4c6ddafa4a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 50.0%\n"
     ]
    }
   ],
   "source": [
    "acc = (predLabels == labels).sum().item() / train_dataloader.batch_size\n",
    "print(f'accuracy is {100* acc:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a8894-9a49-43f4-892d-5c8cca925a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555d574-8791-423b-a813-8afd6533a41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38e144b7-f034-43f4-b7d6-2f10b734ff3a",
   "metadata": {},
   "source": [
    "Import and proceess the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2537ce25-3b23-413f-95d7-61089b9e1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above cells were soume rough work/\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "#reduce the size\n",
    "dataset = DatasetDict({split: dataset[split].select(range(5_000,20_000)) for split in ['train','test']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5bb7cbf-7147-4d80-907d-c1a90859cc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7a3f8ed4e04ecaab55aa4e979f5611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d477b718584808b7d178781aa1cc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a token func that processes each data sample\n",
    "def tokenize_function(one_sample):\n",
    "    return tokenizer(\n",
    "        one_sample['text'],\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True)\n",
    "\n",
    "# apply tokeniation func to dataset (batched for efficiency\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "928fe8d6-55d4-4db3-acec-69f463da6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text pair\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# change format to pytorch tensors\n",
    "tokenized_dataset.set_format('torch',columns=['input_ids', 'attention_mask', 'label','token_type_ids'])\n",
    "\n",
    "# create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(tokenized_dataset['test'], shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78ded345-d650-4cbc-93b2-952e03b3fc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0]),\n",
       " 'input_ids': tensor([[ 101, 1037, 2843,  ...,    0,    0,    0],\n",
       "         [ 101, 2023, 2003,  ...,    0,    0,    0],\n",
       "         [ 101, 2023, 3185,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1045, 2191,  ...,    0,    0,    0],\n",
       "         [ 101, 1037, 2767,  ...,    0,    0,    0],\n",
       "         [ 101, 1008, 2116,  ..., 2156, 1999,  102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e6e1c-f344-4d31-b5cc-8245f45b7aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aac93780-db4e-4b69-b648-a3ad9883eb12",
   "metadata": {},
   "source": [
    "Create and preciision freeze a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70f5dbe1-ea41-415a-87e4-9974743f4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we freeze all of the attention and embedd layers\n",
    "# MLP layer, pooler and final classifier are trainable, rest freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d432811-ca07-4548-9863-d1e4f2508f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForBinaryClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec7a0466-4be8-49e2-8105-3c571e0ff206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer bert.embeddings.word_embeddings.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.embeddings.position_embeddings.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.embeddings.token_type_embeddings.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.embeddings.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.embeddings.LayerNorm.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.0.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.0.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.0.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.0.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.0.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.0.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.0.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.1.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.1.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.1.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.1.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.1.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.1.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.1.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.1.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.2.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.2.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.2.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.2.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.2.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.2.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.2.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.2.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.3.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.3.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.3.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.3.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.3.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.3.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.3.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.3.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.4.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.4.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.4.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.4.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.4.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.4.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.4.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.4.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.5.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.5.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.5.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.5.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.5.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.5.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.5.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.5.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.6.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.6.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.6.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.6.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.6.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.6.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.6.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.6.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.7.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.7.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.7.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.7.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.7.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.7.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.7.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.7.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.8.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.8.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.8.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.8.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.8.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.8.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.8.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.8.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.9.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.9.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.9.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.9.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.9.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.9.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.9.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.9.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.10.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.10.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.10.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.10.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.10.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.10.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.10.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.10.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "--- Layer bert.encoder.layer.11.attention.self.query.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.self.query.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.self.key.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.self.key.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.self.value.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.self.value.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.output.dense.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.output.dense.bias is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.output.LayerNorm.weight is frozen (.required_grad = False)\n",
      "--- Layer bert.encoder.layer.11.attention.output.LayerNorm.bias is frozen (.required_grad = False)\n",
      "+++ Layer bert.encoder.layer.11.intermediate.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.11.intermediate.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.11.output.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.11.output.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.11.output.LayerNorm.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.encoder.layer.11.output.LayerNorm.bias is trainable (.required_grad = True)\n",
      "+++ Layer bert.pooler.dense.weight is trainable (.required_grad = True)\n",
      "+++ Layer bert.pooler.dense.bias is trainable (.required_grad = True)\n",
      "+++ Layer classifier.weight is trainable (.required_grad = True)\n",
      "+++ Layer classifier.bias is trainable (.required_grad = True)\n",
      "\n",
      "\n",
      " there are 52,204,032 (47.68%) frozen params\n",
      "\n",
      "\n",
      " there are 57,279,746 (52.32%) frozen params\n"
     ]
    }
   ],
   "source": [
    "# freeze the attn weights\n",
    "trainParamsCount = 0\n",
    "frozenParamsCount = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if ('attention' in name) or ('embeddings' in name):\n",
    "        param.requires_grad = False\n",
    "        frozenParamsCount += torch.numel(param)\n",
    "        print(f'--- Layer {name} is frozen (.required_grad = {param.requires_grad})')\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        trainParamsCount += torch.numel(param)\n",
    "        print(f'+++ Layer {name} is trainable (.required_grad = {param.requires_grad})')\n",
    "\n",
    "print(f'\\n\\n there are {frozenParamsCount:,} ({frozenParamsCount*100/(frozenParamsCount+trainParamsCount):.2f}%) frozen params')\n",
    "print(f'\\n\\n there are {trainParamsCount:,} ({trainParamsCount*100/(frozenParamsCount+trainParamsCount):.2f}%) frozen params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e3acb7-f832-4282-ba48-d904dfed65ed",
   "metadata": {},
   "source": [
    "WHY DID WE FREEZE THE ATTN LAYERS AND EMBEDD LAYERS, NOT THE MLP??????\n",
    "\n",
    "--- we already have the embeddings, and we trust they are well trained\n",
    "\n",
    "--- attention sublayers are good at idfying distributed patterns across tokens and integrating those patterns\n",
    "\n",
    "\n",
    "--- whereas MLP sublayer is good for idfying spaces in dataset, where we can do some linear clssification and linear separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef27b4c-2202-4776-a259-3e092afd37f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0fcd5e5-a163-4061-b2f3-77c56c7dad0c",
   "metadata": {},
   "source": [
    "Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c49ffcb-0e13-437c-91ae-71f2c650f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 300\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fun = nn.CrossEntropyLoss() # cross entropy for multi class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8e7a7e2-afd9-42d1-8713-efdb3c1327c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0/300, losses (train/test): 0.7703685760498047 / 0.915546715259552 accuracy 0.375 / 0.3125\n",
      "Sample 10/300, losses (train/test): 0.6776073575019836 / 0.6625280380249023 accuracy 0.5 / 0.5625\n",
      "Sample 20/300, losses (train/test): 0.6416552066802979 / 0.6215271949768066 accuracy 0.5625 / 0.6875\n",
      "Sample 30/300, losses (train/test): 0.561294674873352 / 0.5821363925933838 accuracy 0.75 / 0.6875\n",
      "Sample 40/300, losses (train/test): 0.5762384533882141 / 0.5512535572052002 accuracy 0.6875 / 0.8125\n",
      "Sample 50/300, losses (train/test): 0.33386296033859253 / 0.444811075925827 accuracy 0.875 / 0.9375\n",
      "Sample 60/300, losses (train/test): 0.6067137122154236 / 0.4533865451812744 accuracy 0.6875 / 0.8125\n",
      "Sample 70/300, losses (train/test): 0.2847667336463928 / 0.39208781719207764 accuracy 0.875 / 0.8125\n",
      "Sample 80/300, losses (train/test): 0.32578447461128235 / 0.40378129482269287 accuracy 0.875 / 0.875\n",
      "Sample 90/300, losses (train/test): 0.16099172830581665 / 0.3064861297607422 accuracy 0.9375 / 0.875\n",
      "Sample 100/300, losses (train/test): 0.29772043228149414 / 0.16875684261322021 accuracy 0.875 / 0.9375\n",
      "Sample 110/300, losses (train/test): 0.1896573007106781 / 0.08948220312595367 accuracy 1.0 / 1.0\n",
      "Sample 120/300, losses (train/test): 0.21906213462352753 / 0.27261120080947876 accuracy 0.9375 / 0.875\n",
      "Sample 130/300, losses (train/test): 0.30496951937675476 / 0.36983928084373474 accuracy 0.875 / 0.875\n",
      "Sample 140/300, losses (train/test): 0.284746915102005 / 0.35259464383125305 accuracy 0.9375 / 0.8125\n",
      "Sample 150/300, losses (train/test): 0.18584293127059937 / 0.1494952142238617 accuracy 0.875 / 0.9375\n",
      "Sample 160/300, losses (train/test): 0.3058536946773529 / 0.24507607519626617 accuracy 0.875 / 0.875\n",
      "Sample 170/300, losses (train/test): 0.29889166355133057 / 0.27254077792167664 accuracy 0.9375 / 0.875\n",
      "Sample 180/300, losses (train/test): 0.19411376118659973 / 0.2686636745929718 accuracy 0.875 / 0.9375\n",
      "Sample 190/300, losses (train/test): 0.5563979148864746 / 0.2723597288131714 accuracy 0.75 / 0.875\n",
      "Sample 200/300, losses (train/test): 0.21529939770698547 / 0.2503291368484497 accuracy 0.9375 / 0.875\n",
      "Sample 210/300, losses (train/test): 0.07862234115600586 / 0.2502548098564148 accuracy 1.0 / 0.875\n",
      "Sample 220/300, losses (train/test): 0.20713570713996887 / 0.7694011926651001 accuracy 0.9375 / 0.75\n",
      "Sample 230/300, losses (train/test): 0.2657131850719452 / 0.3315761685371399 accuracy 0.9375 / 0.8125\n",
      "Sample 240/300, losses (train/test): 0.4089468717575073 / 0.38580232858657837 accuracy 0.875 / 0.875\n",
      "Sample 250/300, losses (train/test): 0.14121848344802856 / 0.2958901822566986 accuracy 0.9375 / 0.875\n",
      "Sample 260/300, losses (train/test): 0.09432614594697952 / 0.16276612877845764 accuracy 1.0 / 0.9375\n",
      "Sample 270/300, losses (train/test): 0.19256490468978882 / 0.21505285799503326 accuracy 0.9375 / 0.9375\n",
      "Sample 280/300, losses (train/test): 0.4564444124698639 / 0.34699082374572754 accuracy 0.875 / 0.8125\n",
      "Sample 290/300, losses (train/test): 0.22293508052825928 / 0.03845517337322235 accuracy 0.9375 / 1.0\n"
     ]
    }
   ],
   "source": [
    "train_losses = np.zeros(num_samples)\n",
    "train_accuracy = np.zeros(num_samples)\n",
    "test_losses = np.zeros(num_samples)\n",
    "test_accuracy = np.zeros(num_samples)\n",
    "\n",
    "for sampli in range(num_samples):\n",
    "    # get a bathc of data\n",
    "    batch = next(iter(train_dataloader))\n",
    "\n",
    "    # move to GPU\n",
    "    tokenz = batch['input_ids'].to(device)\n",
    "    attn_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "\n",
    "    # clear the prev grads\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # fwd pass and get model preds\n",
    "    logits = model(tokenz, attention_mask = attn_mask)\n",
    "    predLabels = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # calculate and store loss+avg accuracy\n",
    "    loss = loss_fun(logits, labels)\n",
    "    train_losses[sampli] = loss.item()\n",
    "    train_accuracy[sampli] = (predLabels == labels).sum().item() / train_dataloader.batch_size\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    #test the model and report losses every k samples\n",
    "    if sampli%10 == 0:\n",
    "        # evaluate using test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = next(iter(test_dataloader))\n",
    "            tokenz = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # fwd pass and get model preds\n",
    "            logits = model(tokenz, attention_mask = attn_mask)\n",
    "            predLabels = torch.argmax(logits, dim=1)\n",
    "\n",
    "            loss = loss_fun(logits, labels)\n",
    "            test_losses[sampli] = loss.item()\n",
    "            test_accuracy[sampli] = (predLabels == labels).sum().item() / train_dataloader.batch_size\n",
    "\n",
    "            print(f'Sample {sampli}/{num_samples}, losses (train/test): {train_losses[sampli]} / {test_losses[sampli]} accuracy {train_accuracy[sampli]} / {test_accuracy[sampli]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb53b0-2563-4eb7-9577-c83a493c14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
