{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa16c95e-c092-4589-b31e-9954478b9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use BERT model and add a final classification layer to classify into 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae425772-78a9-4ea1-b9b8-dd0a49f08589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets huggingface_hub fsspec\n",
    "from datasets import load_dataset, DatasetDict\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e474789-eabf-48d2-b128-c52a6e565e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ad6082-a81d-460c-b6db-d6f623db2b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549634e6f5cc44e7b7fd4d5e4aa6c76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1b5241-1830-4b46-941c-60d659c45bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664d8718-e471-42ad-a6db-17205443176d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert\n",
    "# note the namings are diff to OpenAI\n",
    "# we have a pooler layer as final layer which has 768x768, not the vocab_size, bcoz this is for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce06d56-8d79-412d-8708-932d948c139c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.embeddings.dropout.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566f5cb-f7a4-49c5-8128-c14220843849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82616573-dad4-4bf5-899d-6ef13d386683",
   "metadata": {},
   "source": [
    "More on model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e25885-a0cd-4f3e-a62e-9541cff3fdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5672, 2033, 2007, 2151, 3793, 2017, 2066, 1012,  102]],\n",
       "       device='mps:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Replace me with any text you like.'\n",
    "tokens = tokenizer(text, return_tensors='pt').to(device)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f68b88-a049-4c85-a4a2-8b51e913574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model output using sepcific inputs\n",
    "output = bert(\n",
    "    input_ids = tokens['input_ids'],\n",
    "    attention_mask = tokens['attention_mask']\n",
    ")\n",
    "\n",
    "# better way\n",
    "output = bert(**tokens) #dict unpacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b6c631-c688-4fc0-b07c-cbb6b80fb322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'last_hidden_state',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pooler_output',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1f8e16-a72d-4c9a-9f6b-7841a5364961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35183fa5-19a9-43bc-99d4-1f6875ff1826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape #output of final linear layer of bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbdc8d96-7314-49da-a41c-ce8e21747ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# can we get text output?  NOOOO bcoz this is not for text gen\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(tokens, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1965\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1964\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1965\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1967\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "# can we get text output?  NOOOO bcoz this is not for text gen\n",
    "bert.generate(tokens, max_length=100,do_sample=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25478ae9-ccd8-432a-a371-e4878b565124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "989226dd-3c15-4de8-b320-20bc277f084d",
   "metadata": {},
   "source": [
    "CREATE an LLM model using pretrained BERT with a new head that does the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a535711-51fa-4228-9cc8-89ff145e1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForBinaryClassification(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForBinaryClassification, self).__init__()\n",
    "\n",
    "        # Load the pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        #classification head that converts 678-d pooled output into 2 final outuputs\n",
    "        self.classifier = nn.Linear(768,2)\n",
    "        self.dropout = nn.Dropout(self.bert.embeddings.dropout.p) #10%\n",
    "\n",
    "        #init the weights and biases\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "        # fwd pas through the downloaded(pretrained) BERT\n",
    "        outputs = self. bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_idsn = token_type_ids)\n",
    "\n",
    "        # extract the pooled output and apply dropout\n",
    "        pooled_output  = self.dropout(outputs.pooler_output)\n",
    "\n",
    "        # final push through classification layer\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aee2ead5-cd42-4258-a760-f463f8e2ef8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228a5ce8ad914c04a7d887da154cfa4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0514, -1.0165]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of model and test it\n",
    "model = BertForBinaryClassification().to(device)\n",
    "\n",
    "tokens = tokenizer(text, return_tensors='pt').to(device)\n",
    "out = model(**tokens)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9417e80-c76a-4b71-9a2d-56433c43a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output of the model is not nexxt token prediction\n",
    "# its a binary classification\n",
    "# now we train the model, to correspond this output to positive and -ve reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf519e99-6ddf-42b3-b409-e69bfa98a078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "731e84b8-5bc9-4044-b930-37e28f53c99a",
   "metadata": {},
   "source": [
    "Import the datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "659e06ba-5637-421d-847f-4df0b81a0355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "# this is a special dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7617d02c-1361-4943-bcb3-2ef4ff7528b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"S.S. Van Dine must have been a shrewd businessman in dealing with Hollywood. Most of the film series' from the studio days were usually confined to one or two studios. But apparently Van Dine must have sold his rights to each book about Philo Vance one at a time. Note that Paramount, MGM, Warner Brothers, and more all released Philo Vance films. Only Tarzan seemed to get around Hollywood more.<br /><br />MGM produced the Garden Murder Case and starred Edmund Lowe as the fashionable detective. Of course MGM had the screen's original Philo under contract at the time, but Bill Powell was busy doing The Thin Man at the time and I guess Louis B. Mayer decided to concentrate him there.<br /><br />Edmund Lowe is a pretty acceptable Philo Vance. Lowe had started out pretty big at the tail end of the silent era with What Price Glory and then with a string of films with Victor McLaglen with their Flagg and Quirt characters. But after McLaglen got his Oscar for The Informer, Lowe seemed to fade into the B picture market. <br /><br />The Garden Murder Case involves three separate victims, Douglas Walton, Gene Lockhart, and Frieda Inescourt. The sinister atmosphere around the perpetrator kind of gives it away, the mystery is really how all the killings are connected and how they are accomplished.<br /><br />I will say this though. Vance takes a very big chance in exposing the villain and the last 15 minutes are worthy of Hitchcock.\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][24000]\n",
    "# we have labels 0 and 1 -ve and +ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aedbf-7bd8-48fd-a5f9-1b05c0a0e778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5aa9d7-ee94-486f-90ae-af758488e1c0",
   "metadata": {},
   "source": [
    "its a lot of data, lets take a small sample and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "596988a7-712f-42a3-b349-8b154b9a00e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this special dict has a method called select which selects data points\n",
    "dataset['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "389d4a21-5add-49cb-9fc5-07818ca12103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset['train'] ois organised in a way that first half is all 0s and next half all 1s\n",
    "# so we need to reduce dataset size while:\n",
    "#    1) including both categories\n",
    "#    2) preserving only 'train' and 'test (dont use unsupervised)\n",
    "smalldata = DatasetDict({split: dataset[split].select(range(10000,15000)) for split in ['train','test']})\n",
    "smalldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb47c5d-ba9e-437b-9e70-36fc8b30feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf8d184-5db0-4edb-b45c-3f91ed3197e6",
   "metadata": {},
   "source": [
    "Tokenizing the text with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77daa9ab-ba33-4439-bb4c-b1e0a89423a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this text, we have variable sequences\n",
    "# we need to deal with this because LMs not designed to handle batches with seq of diff lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "260597bb-c2b1-45fa-a513-cba703f71863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Naive' tokenization (N=363):\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n",
      "\n",
      "Better tokenization (N=512):\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# this works....\n",
    "first_try = tokenizer(dataset['train'][0]['text'])\n",
    "\n",
    "# but this is better bcz reviews have diff lengths\n",
    "better  = tokenizer(\n",
    "    dataset['train'][0]['text'], # the text to tokenize\n",
    "    max_length = 512,\n",
    "    padding = 'max_length', # usning pad_token to reach max_len\n",
    "    truncation=True) # cut out tokens > max_len\n",
    "\n",
    "print(f\"'Naive' tokenization (N={len(first_try['input_ids'])}):\")\n",
    "print(f\"{first_try['input_ids']}\")\n",
    "\n",
    "print(f\"\\nBetter tokenization (N={len(better['input_ids'])}):\")\n",
    "print(f\"{better['input_ids']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed9962a7-e89c-4e9c-a89b-328e248d137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a token func that processes each data sample\n",
    "def tokenize_function(one_sample):\n",
    "    return tokenizer(\n",
    "        one_sample['text'],\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True)\n",
    "\n",
    "# apply tokeniation func to dataset (batched for efficiency\n",
    "tokenized_dataset = smalldata.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc083bbb-cacf-4fc1-ae90-d8f4a58230d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "204c55a4-603c-4b5b-b44f-4e4bdfef45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text pair\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# change format to pytorch tensors\n",
    "tokenized_dataset.set_format('torch',columns=['input_ids', 'attention_mask', 'label','token_type_ids'])\n",
    "\n",
    "# create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(tokenized_dataset['test'], shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc2d5faa-ee02-4b9d-a27b-1b68653bbb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "762d31c4-cc24-4cc8-9ae0-0b7a2e725129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0]),\n",
       " 'input_ids': tensor([[ 101, 2023, 3185,  ...,    0,    0,    0],\n",
       "         [ 101, 2023, 3185,  ...,    0,    0,    0],\n",
       "         [ 101, 1000, 1996,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2044, 3974,  ...,    0,    0,    0],\n",
       "         [ 101, 2023, 3185,  ...,    0,    0,    0],\n",
       "         [ 101, 1045, 2787,  ...,    0,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = next(iter(train_dataloader))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4756988a-822f-4d08-a109-5e99575044e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(X['input_ids'].shape)\n",
    "print(X['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937117a-8d55-44dc-a354-f0cc0bccb9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3167a0b-14bf-4178-aefa-b72f2fcaffbc",
   "metadata": {},
   "source": [
    "Now fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cb1c75e-7b86-436f-99be-6969386d58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fun = nn.CrossEntropyLoss() # cross entropy for multi class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5157d0d-e8cc-4cb9-8592-af7255a6a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a bathc of data\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# move to GPU\n",
    "tokenz = batch['input_ids'].to(device)\n",
    "attn_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['label'].to(device)\n",
    "\n",
    "# clear the prev grads\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# fwd pass and get model preds\n",
    "logits = model(tokenz, attention_mask = attn_mask)\n",
    "predLabels = torch.argmax(logits, dim=1)\n",
    "\n",
    "# calculate and store loss+avg accuracy\n",
    "loss = loss_fun(logits, labels)\n",
    "train_accuracy = (predLabels == labels).sum().item() / train_dataloader.batch_size\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f322088f-152d-4fe7-a7b7-823704a47b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits are of size torch.Size([16, 2]) and are:\n",
      " tensor([[ 0.4223, -0.5061],\n",
      "        [ 0.4937, -1.1348],\n",
      "        [ 0.6020, -1.4387],\n",
      "        [ 1.1124, -1.0846],\n",
      "        [ 1.1038, -1.3456],\n",
      "        [ 1.0491, -1.6837],\n",
      "        [ 1.2433, -1.4893],\n",
      "        [ 0.4244, -1.1602],\n",
      "        [ 0.6467, -0.8793],\n",
      "        [ 0.7566, -0.9556],\n",
      "        [ 0.7556, -1.2713],\n",
      "        [ 0.8743, -1.4579],\n",
      "        [ 0.9014, -1.2644],\n",
      "        [ 0.3062, -1.1704],\n",
      "        [ 0.6058, -0.8031],\n",
      "        [ 0.2592, -1.0258]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'logits are of size {logits.shape} and are:\\n', logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcdd5971-66b4-46a7-8f85-7d0b98badf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model preds: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='mps:0')\n",
      "True labels: tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print('Model preds:', predLabels)\n",
    "print('True labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adaa7f-a02a-4afd-82ac-4c6ddafa4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
