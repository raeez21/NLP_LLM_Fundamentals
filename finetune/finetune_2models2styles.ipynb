{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab42887-6549-4f23-a1d4-58b1c31ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this uses GPT-neo and is finetuned on 2 different styles\n",
    "# alice in wonderland and edgar alan poe style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723e615c-6800-4c6b-ae62-880fe44e9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchinfo import summary\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84ded28-3b33-4181-a935-4fbcb24a9085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|█| 160/160 [00:00<00:00, 1711.82it/s, Materializing param=\n",
      "\u001b[1mGPTNeoForCausalLM LOAD REPORT\u001b[0m from: EleutherAI/gpt-neo-125m\n",
      "Key                                                   | Status     |  | \n",
      "------------------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
      "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Loading weights: 100%|█| 160/160 [00:00<00:00, 1159.89it/s, Materializing param=\n",
      "\u001b[1mGPTNeoForCausalLM LOAD REPORT\u001b[0m from: EleutherAI/gpt-neo-125m\n",
      "Key                                                   | Status     |  | \n",
      "------------------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
      "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Eletuther's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n",
    "\n",
    "# load in 2 GPTneos and push to GPU\n",
    "modelAlice = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "modelEdgar = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "modelAlice = modelAlice.to(device)\n",
    "modelEdgar = modelEdgar.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bc6ff-c693-4857-b218-c77ebf5b4590",
   "metadata": {},
   "source": [
    "inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3008d172-3ea8-45c5-842d-e7df7a280b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAlice\n",
    "# here the q,k,v matrix is seperate in attenion unlike OpenAI GPT2 ewhere it was all 1 big matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876ea822-dc8a-4b27-ba1b-75778fad035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessign a particular weights matrix\n",
    "modelAlice.transformer.h[3].attn.attention.k_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0ede91-cc38-455d-8711-60d7817d943a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAlice.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c55c94-232d-4397-b6c7-5079fa596457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #\n",
       "==================================================================================================================================\n",
       "GPTNeoForCausalLM                                       [1, 9]                    --                        --\n",
       "├─GPTNeoModel: 1-1                                      [1, 9]                    --                        --\n",
       "│    └─Embedding: 2-1                                   [1, 9]                    [1, 9, 768]               38,597,376\n",
       "│    └─Embedding: 2-2                                   [1, 9]                    [1, 9, 768]               1,572,864\n",
       "│    └─Dropout: 2-3                                     [1, 9, 768]               [1, 9, 768]               --\n",
       "│    └─ModuleList: 2-4                                  --                        --                        --\n",
       "│    │    └─GPTNeoBlock: 3-1                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-2                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-3                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-4                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-5                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-6                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-7                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-8                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-9                            [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-10                           [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-11                           [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    │    └─GPTNeoBlock: 3-12                           [1, 9, 768]               [1, 9, 768]               7,085,568\n",
       "│    └─LayerNorm: 2-5                                   [1, 9, 768]               [1, 9, 768]               1,536\n",
       "├─Linear: 1-2                                           [1, 9, 768]               [1, 9, 50257]             38,597,376\n",
       "==================================================================================================================================\n",
       "Total params: 163,795,968\n",
       "Trainable params: 163,795,968\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 163.80\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 11.08\n",
       "Params size (MB): 655.18\n",
       "Estimated Total Size (MB): 666.27\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model summar y\n",
    "x = tokenizer.encode('What did the Red Queen say to Alice?', return_tensors='pt').to(device)\n",
    "summary(modelAlice, input_data=x, col_names=['input_size', 'output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a88ff1-af74-454b-bc1d-75c17cd79114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Embedding:\n",
      " tensor([[ 0.1709, -0.7383,  0.4277,  ...,  0.0840,  0.5820, -0.3457],\n",
      "        [ 0.2070, -0.6055,  0.4590,  ...,  0.1562,  0.4883, -0.2363],\n",
      "        [ 0.2324, -0.6367,  0.3262,  ...,  0.2236,  0.7500, -0.2354],\n",
      "        ...,\n",
      "        [ 0.7734, -1.1406,  0.6523,  ...,  0.2832,  0.9258, -0.5547],\n",
      "        [ 0.3906, -0.8438,  0.5117,  ...,  0.0148,  0.6992, -0.2383],\n",
      "        [ 0.2734, -0.7148,  0.2949,  ...,  0.1748,  0.4043, -0.3105]],\n",
      "       device='mps:0')\n",
      "\n",
      "\n",
      " ** Unmbedding:\n",
      " tensor([[ 0.1709, -0.7383,  0.4277,  ...,  0.0840,  0.5820, -0.3457],\n",
      "        [ 0.2070, -0.6055,  0.4590,  ...,  0.1562,  0.4883, -0.2363],\n",
      "        [ 0.2324, -0.6367,  0.3262,  ...,  0.2236,  0.7500, -0.2354],\n",
      "        ...,\n",
      "        [ 0.7734, -1.1406,  0.6523,  ...,  0.2832,  0.9258, -0.5547],\n",
      "        [ 0.3906, -0.8438,  0.5117,  ...,  0.0148,  0.6992, -0.2383],\n",
      "        [ 0.2734, -0.7148,  0.2949,  ...,  0.1748,  0.4043, -0.3105]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# are the embedds and unembedds tied?\n",
    "print('** Embedding:\\n', modelAlice.transformer.wte.weight.detach())\n",
    "print('\\n\\n ** Unmbedding:\\n', modelAlice.lm_head.weight.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7da18-9778-422a-a145-c815048f631d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b158b53-7f75-4b9a-a4f5-3c8cade5df17",
   "metadata": {},
   "source": [
    "Explore the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c829ea41-c4e6-4612-9c62-03bed5059042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has 50,257 tokens. \n",
      "A few radnom tokens:\n",
      "\n",
      "Token 31680 is \" caster\"\n",
      "Token  8715 is \"dated\"\n",
      "Token 16206 is \"oven\"\n",
      "Token  5710 is \" dropped\"\n",
      "Token 42713 is \" Wonderland\"\n",
      "Token 41404 is \" veterinarian\"\n",
      "Token 40373 is \"ISSION\"\n",
      "Token 38888 is \"voice\"\n",
      "Token 34313 is \" politely\"\n",
      "Token  6091 is \" Haw\"\n",
      "Token 33011 is \"olla\"\n",
      "Token  7304 is \"pload\"\n",
      "Token  5766 is \" factor\"\n",
      "Token 18636 is \"ipes\"\n",
      "Token 23916 is \" cans\"\n",
      "Token 13623 is \" rats\"\n",
      "Token 13054 is \" barrier\"\n",
      "Token 14461 is \" marginal\"\n",
      "Token 34357 is \" craving\"\n",
      "Token 13689 is \"Earlier\"\n",
      "Token 30471 is \" Subaru\"\n",
      "Token 46668 is \" ti\"\n",
      "Token  9391 is \" ingredients\"\n",
      "Token   404 is \"op\"\n",
      "Token 30660 is \" Ogre\"\n",
      "Token  9332 is \" efficiency\"\n",
      "Token 15381 is \" shifts\"\n",
      "Token 22501 is \"strous\"\n",
      "Token 24146 is \" Sniper\"\n",
      "Token 39550 is \" attaching\"\n"
     ]
    }
   ],
   "source": [
    "# a bit about their tokenizer\n",
    "print(f'Tokenizer has {tokenizer.vocab_size:,} tokens. \\nA few radnom tokens:\\n')\n",
    "for i in range(30):\n",
    "    randtok = torch.randint(tokenizer.vocab_size,(1,))\n",
    "    print(f'Token {randtok[0]:5} is \"{tokenizer.decode(randtok)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd90b9c-aaf1-4e8e-b2d8-62a1cbbaedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizer is smae as GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1646edf-8251-4342-8ea2-23bfe693cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66693abe-ef12-44c7-aa75-73fed1b09bd2",
   "metadata": {},
   "source": [
    "import and process texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928a701e-0666-4430-bb6f-68511c4bdd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52954 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# throught the looking glass (aka alice in wonderland)\n",
    "text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n",
    "aliceTokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "# edgar allan Poe\n",
    "text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n",
    "edgarTokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f25bad2-45ef-4de1-9612-344a03a0270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allice in wonderland has 52,954 tokens.\n",
      "Edgar Alan Poe has 197,306 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'Allice in wonderland has {len(aliceTokens):,} tokens.')\n",
    "print(f'Edgar Alan Poe has {len(edgarTokens):,} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264d3b7-f5c4-472f-b899-f0490d228d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a070265-5b05-4fff-b1cb-948612655ee9",
   "metadata": {},
   "source": [
    "Prepare for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab3d3f7b-23ab-4854-9a03-7a8b227d041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALICE optmizer\n",
    "optimizerAlice = torch.optim.AdamW(modelAlice.parameters(), lr=5e-5, weight_decay=.01)\n",
    "optimizerEdgar = torch.optim.AdamW(modelEdgar.parameters(), lr=5e-5, weight_decay=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7cd0c8a-74f8-401e-9f1e-38f899f5c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 16\n",
    "num_samples = 476\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5bc86-4c3a-406d-bb44-6f4c95a58741",
   "metadata": {},
   "source": [
    "fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79e8d21d-e67a-4aef-81f0-c3999ee2ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 0/476, losses (Alice/eDgar): 2.42812442779541 / 2.6880459785461426\n",
      "Sample: 25/476, losses (Alice/eDgar): 1.8133280277252197 / 2.61869740486145\n",
      "Sample: 50/476, losses (Alice/eDgar): 1.6113585233688354 / 2.7559566497802734\n",
      "Sample: 75/476, losses (Alice/eDgar): 1.3369135856628418 / 2.6015217304229736\n",
      "Sample: 100/476, losses (Alice/eDgar): 1.0890058279037476 / 2.6897029876708984\n",
      "Sample: 125/476, losses (Alice/eDgar): 0.7399564385414124 / 2.6269607543945312\n",
      "Sample: 150/476, losses (Alice/eDgar): 0.6405403017997742 / 2.6760103702545166\n",
      "Sample: 175/476, losses (Alice/eDgar): 0.5025162100791931 / 2.5033085346221924\n",
      "Sample: 200/476, losses (Alice/eDgar): 0.34113582968711853 / 2.642308473587036\n",
      "Sample: 225/476, losses (Alice/eDgar): 0.22491006553173065 / 2.7783093452453613\n",
      "Sample: 250/476, losses (Alice/eDgar): 0.24024367332458496 / 2.6661019325256348\n",
      "Sample: 275/476, losses (Alice/eDgar): 0.19655752182006836 / 2.6313257217407227\n",
      "Sample: 300/476, losses (Alice/eDgar): 0.17590542137622833 / 2.5445780754089355\n",
      "Sample: 325/476, losses (Alice/eDgar): 0.16068294644355774 / 2.6870787143707275\n",
      "Sample: 350/476, losses (Alice/eDgar): 0.1841307282447815 / 2.56144118309021\n",
      "Sample: 375/476, losses (Alice/eDgar): 0.14415058493614197 / 2.7121682167053223\n",
      "Sample: 400/476, losses (Alice/eDgar): 0.12639814615249634 / 2.706012487411499\n",
      "Sample: 425/476, losses (Alice/eDgar): 0.14089904725551605 / 2.5741569995880127\n",
      "Sample: 450/476, losses (Alice/eDgar): 0.13120850920677185 / 2.6707968711853027\n",
      "Sample: 475/476, losses (Alice/eDgar): 0.13304410874843597 / 2.7237491607666016\n"
     ]
    }
   ],
   "source": [
    "tokenProbs = np.zeros((num_samples,3))\n",
    "\n",
    "lossAlice = np.zeros(num_samples)\n",
    "lossEdgar = np.zeros(num_samples)\n",
    "\n",
    "for sampli in range(num_samples):\n",
    "    # init batch losses to accumulate\n",
    "\n",
    "    # ALICE fine tuning\n",
    "    # get a batch of data\n",
    "    ix = torch.randint(len(aliceTokens)-seq_len, size = (batch_size,))\n",
    "    X = aliceTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n",
    "\n",
    "    #fwd pass and get loss\n",
    "    modelAlice.zero_grad()\n",
    "    outputs = modelAlice(X, labels=X)\n",
    "\n",
    "    # backprop and store loss\n",
    "    outputs.loss.backward()\n",
    "    optimizerAlice.step()\n",
    "    lossAlice[sampli] = outputs.loss.item()\n",
    "\n",
    "\n",
    "    #EDGAR fine tuning\n",
    "    ix = torch.randint(len(edgarTokens)-seq_len, size = (batch_size,))\n",
    "    X = edgarTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n",
    "\n",
    "    #fwd pass and get loss\n",
    "    modelEdgar.zero_grad()\n",
    "    outputs = modelEdgar(X, labels=X)\n",
    "\n",
    "    # backprop and store loss\n",
    "    outputs.loss.backward()\n",
    "    optimizerAlice.step()\n",
    "    lossEdgar[sampli] = outputs.loss.item()\n",
    "\n",
    "    if sampli%25==0:\n",
    "        print(f'Sample: {sampli}/{num_samples}, losses (Alice/eDgar): {lossAlice[sampli]} / {lossEdgar[sampli]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da7eabc7-d70d-481f-8b82-f1b738b7810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alice book had better learning than Edgar\n",
    "# bcoz Alice is from 1 single book, edgar is from colelction of his poems\n",
    "# alice book is more homogenous in terms of writing style and content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d1cb4-86fc-4357-af8a-1a4db34ef172",
   "metadata": {},
   "source": [
    "Qualitative assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bed6e759-5a32-45f0-87e5-e16cb5215c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Alice model says:\n",
      "What did the Red Queen say to Alice?”\n",
      "\n",
      "“Yes, she said ‘Will you be _very_ pleased? She is such a dear\n",
      "quiet thing,’” said Alice, rubbing her eyes, and asking herself how\n",
      "anybody would have understood the young lady thought she was\n",
      "replicating her thoughts.\n",
      "\n",
      "“She is an only child,” the Duchess explained.\n",
      "\n",
      "“And because of her,” said the Duchess, “it _must_ have been for\n",
      "her having been denied the opportunity of\n",
      "\n",
      "\n",
      "** Edgar model says:\n",
      "What did the Red Queen say to Alice?\n",
      "\n",
      "The Red Queen’s letter begins by asking Alice to read, and she asks Alice, in her most direct and obvious way to have “this great day”, to read it; and it ends by asking Alice to get it out of there while Alice continues.\n",
      "HENRY, WENDY, BLESSING, and BEER. The two men meet in a field, each wearing something blue around his head.\n",
      "\n",
      "A. The two men first meet Alice’s brother, who tells them they can’t read anything they’re\n"
     ]
    }
   ],
   "source": [
    "# model summar y\n",
    "x = tokenizer.encode('What did the Red Queen say to Alice?', return_tensors='pt').to(device)\n",
    "\n",
    "outAlice = modelAlice.generate(x,max_new_tokens=120, do_sample=True, pad_token_id=50257)\n",
    "outEdgar = modelEdgar.generate(x,max_new_tokens=120, do_sample=True, pad_token_id=50257)\n",
    "\n",
    "\n",
    "#print both models outputs\n",
    "print('** Alice model says:')\n",
    "print(tokenizer.decode(outAlice[0].cpu()))\n",
    "\n",
    "print('\\n\\n** Edgar model says:')\n",
    "print(tokenizer.decode(outEdgar[0].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b58fc2-aeb5-4e07-bec3-a3904ea72280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
