{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65871eaa-5bb3-4010-9658-acded5f529ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebooks explainns gradient clipping and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08314539-decd-463e-9b22-3fa332077a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91126b86-d3e6-4292-bca8-7856eeec1969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97ec752-3240-4b93-8953-9fbb6cdae9fa",
   "metadata": {},
   "source": [
    "Simple demo of grad clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "663339a7-55ad-4091-90f2-a367ae0561fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CLIPPING\n",
      "Gradient vals: [-2.0, 6.599999904632568, 4.0, -10.0, 6.0, -4.0, -8.0, -10.0, 3.0]\n",
      "Gradeient norm: 19.712\n",
      "AFTER CLIPPING\n",
      "Gradient vals: [-0.10146141052246094, 0.3348226547241211, 0.20292282104492188, -0.5073070526123047, 0.3043842315673828, -0.20292282104492188, -0.40584564208984375, -0.5073070526123047, 0.1521921157836914]\n",
      "Gradeient norm: 1.000\n"
     ]
    }
   ],
   "source": [
    "# tensor with gradients (like a weight matrix)\n",
    "w = torch.tensor([[-1,3.3,2,-5,3,-2,-4,-5,1.5]], requires_grad = True) # consider this is weights in a model\n",
    "\n",
    "# loss is sum of squares (L2), a dummy loss fun\n",
    "loss = (w**2).sum()\n",
    "\n",
    "# backprop\n",
    "loss.backward()\n",
    "\n",
    "#print grad and their norm before clipping\n",
    "print('BEFORE CLIPPING')\n",
    "print(f'Gradient vals: {w.grad[0].tolist()}')\n",
    "print(f'Gradeient norm: {torch.norm(w.grad):.3f}')\n",
    "\n",
    "#apply grad clipping\n",
    "preClipVals = w.grad[0].detach() + 0\n",
    "nn.utils.clip_grad_norm_([w],max_norm=1) #this is an inplace fun\n",
    "\n",
    "print('AFTER CLIPPING')\n",
    "print(f'Gradient vals: {w.grad[0].tolist()}')\n",
    "print(f'Gradeient norm: {torch.norm(w.grad):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd14e924-df33-4645-9859-5449fe3a283c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1015,  0.3348,  0.2029, -0.5073,  0.3044, -0.2029, -0.4058, -0.5073,\n",
       "         0.1522])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a130161-1bb4-4846-ac0c-118e664ec6c6",
   "metadata": {},
   "source": [
    "![title](../images/PrePostClip.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a4d9e3-efb0-4dca-a136-063512fb995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation of pre and post clipped grad values is 1\n",
    "# that means the indiviidual values doesnt change, the entire matrix of weights values is shrinking down\n",
    "# such that the norm of their grad is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fc38a-ae54-4294-8c32-3f48ad20a675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
